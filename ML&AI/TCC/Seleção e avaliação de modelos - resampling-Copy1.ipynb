{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "302f1b04-204d-442f-8a53-934e031a4aff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from ydata_profiling import ProfileReport\n",
    "import warnings\n",
    "import time\n",
    "import os\n",
    "import json\n",
    "import zipfile\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import plotly.express as px\n",
    "import plotly.subplots as sp\n",
    "import plotly.graph_objects as go\n",
    "from imblearn.pipeline import Pipeline\n",
    "from imblearn.over_sampling import SMOTE\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler, StandardScaler, LabelEncoder\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split,\n",
    "    GridSearchCV,\n",
    "    cross_val_score,\n",
    "    StratifiedKFold,\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from sklearn.manifold import TSNE\n",
    "from imblearn.base import BaseSampler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "\n",
    "\n",
    "from sklearn.model_selection import KFold, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, FunctionTransformer, OneHotEncoder\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import (\n",
    "    confusion_matrix,\n",
    "    accuracy_score,\n",
    "    classification_report,\n",
    "    roc_auc_score,\n",
    "    f1_score,\n",
    "    precision_score,\n",
    "    recall_score,\n",
    ")\n",
    "from autorank import autorank, plot_stats, create_report, latex_table\n",
    "\n",
    "import sdgym\n",
    "from sdv.metadata import SingleTableMetadata\n",
    "from sdv.single_table import (\n",
    "    CTGANSynthesizer,\n",
    "    TVAESynthesizer,\n",
    "    GaussianCopulaSynthesizer,\n",
    "    CopulaGANSynthesizer,\n",
    ")\n",
    "from sdv.lite import SingleTablePreset\n",
    "from sdv.evaluation.single_table import (\n",
    "    evaluate_quality,\n",
    "    get_column_plot,\n",
    "    get_column_pair_plot,\n",
    "    run_diagnostic,\n",
    ")\n",
    "from sdv.sampling import Condition\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "sns.set(style=\"darkgrid\", font_scale=0.5)\n",
    "custom_palette = [\"#8b4513\", \"#90ee90\", \"#545454\", \"#6a287e\", \"#f0be00\"]\n",
    "sns.set_palette(custom_palette)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7f8bbd1-ceaf-4339-9437-4b61aca306b7",
   "metadata": {},
   "source": [
    "## Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c3bbce24-efc4-495f-a655-7605696a000a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def dummy(df, columns):\n",
    "    dummy_variables = []\n",
    "    for column in columns:\n",
    "        dummies = pd.get_dummies(df[column], prefix=column)\n",
    "        dummy_variables.append(dummies)\n",
    "    return dummy_variables\n",
    "\n",
    "def dummy_transform(df, columns):\n",
    "    dummy_variables = dummy(df, cat_features)\n",
    "    df = pd.concat([df] + dummy_variables, axis=1)\n",
    "    df = df.drop(cat_features, axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3084bd82-c3b2-4198-87aa-ee4621466dc5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def dummy_transform(df, columns):\n",
    "#     dummy_variables = []\n",
    "    \n",
    "#     for column in columns:\n",
    "#         dummies = pd.get_dummies(X[column], prefix=column)\n",
    "#         dummy_variables.append(dummies)\n",
    "#     result = pd.concat([df] + dummy_variables, axis=1)\n",
    "#     result = result.drop(columns, axis=1)\n",
    "    \n",
    "#     return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "17029505-1736-4f16-aefe-b446cf91e859",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode_columns(df, columns_to_encode):\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    for column in columns_to_encode:\n",
    "        if column in df.columns:\n",
    "            df[column] = label_encoder.fit_transform(df[column])\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1260625d-03d7-42fa-b179-177ffc251538",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_std_results(mean_df, std_df):\n",
    "    concatenated_results = []\n",
    "\n",
    "    for index, row_mean in mean_df.iterrows():\n",
    "        classificador = row_mean['Classification']\n",
    "        row_std = std_df.loc[std_df['Classification'] == classificador].squeeze()\n",
    "\n",
    "        accuracy = f\"{row_mean['Accuracy']:.4f} +- {row_std['Accuracy']:.2f}\"\n",
    "        precision = f\"{row_mean['Precision']:.4f} +- {row_std['Precision']:.2f}\"\n",
    "        recall = f\"{row_mean['Recall']:.4f} +- {row_std['Recall']:.2f}\"\n",
    "        f1 = f\"{row_mean['F1 Score']:.4f} +- {row_std['F1 Score']:.2f}\"\n",
    "        roc_auc = f\"{row_mean['ROC/AUC']:.4f} +- {row_std['ROC/AUC']:.2f}\"\n",
    "\n",
    "        concatenated_results.append({\n",
    "            \"Classification\": classificador,\n",
    "            \"Accuracy\": accuracy,\n",
    "            \"Precision\": precision,\n",
    "            \"Recall\": recall,\n",
    "            \"F1 Score\": f1,\n",
    "            \"ROC/AUC\": roc_auc\n",
    "        })\n",
    "\n",
    "    return pd.DataFrame(concatenated_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "80b9afee-92f1-4878-b256-ab77c0c0c647",
   "metadata": {},
   "outputs": [],
   "source": [
    "class _ParamsValidationMixin:\n",
    "    \"\"\"Mixin class to validate parameters.\"\"\"\n",
    "\n",
    "    def _validate_params(self):\n",
    "        \"\"\"Validate types and values of constructor parameters.\n",
    "\n",
    "        The expected type and values must be defined in the `_parameter_constraints`\n",
    "        class attribute, which is a dictionary `param_name: list of constraints`. See\n",
    "        the docstring of `validate_parameter_constraints` for a description of the\n",
    "        accepted constraints.\n",
    "        \"\"\"\n",
    "        if hasattr(self, \"_parameter_constraints\"):\n",
    "            validate_parameter_constraints(\n",
    "                self._parameter_constraints,\n",
    "                self.get_params(deep=False),\n",
    "                caller_name=self.__class__.__name__,\n",
    "            )\n",
    "\n",
    "class SDVPipelineTBTR(_ParamsValidationMixin, BaseSampler):\n",
    "    _sampling_type = \"bypass\"\n",
    "\n",
    "    _parameter_constraints = {\n",
    "        \"metadata\": [object],\n",
    "        \"target\": [str],\n",
    "        \"num_features\": [list],\n",
    "        \"cat_features\": [list],\n",
    "    }\n",
    "    \n",
    "    def __init__(self, metadata, target, num_features, cat_features):\n",
    "        print(\"Init\")\n",
    "        self.metadata = metadata\n",
    "        self.target = target\n",
    "        self.num_features = num_features\n",
    "        self.cat_features = cat_features\n",
    "        self.synthesizer = SingleTablePreset(self.metadata, name=\"FAST_ML\")\n",
    "        super().__init__()\n",
    "\n",
    "    def _fit_resample(self, X, y):\n",
    "        print(\"Fit\")\n",
    "        df_train = pd.concat([X, y], axis=1)\n",
    "        self.synthesizer.fit(df_train)\n",
    "\n",
    "        class_counts = y.value_counts()\n",
    "        minority_class = class_counts.idxmin()\n",
    "        synthetic_samples_needed = class_counts.max() - class_counts.min()\n",
    "\n",
    "        if minority_class == 0:\n",
    "            balanced_conditions_0 = Condition(\n",
    "                num_rows=synthetic_samples_needed,\n",
    "                column_values={self.target: 0},\n",
    "            )\n",
    "            df_synth = self.synthesizer.sample_from_conditions(\n",
    "                conditions=[balanced_conditions_0]\n",
    "            )\n",
    "        elif minority_class == 1:\n",
    "            balanced_conditions_1 = Condition(\n",
    "                num_rows=synthetic_samples_needed,\n",
    "                column_values={self.target: 1},\n",
    "            )\n",
    "            df_synth = self.synthesizer.sample_from_conditions(\n",
    "                conditions=[balanced_conditions_1]\n",
    "            )\n",
    "        \n",
    "        # onehotencoder\n",
    "        # dummy_variables = dummy(df_synth, self.cat_features)\n",
    "        # df_synth = pd.concat([df_synth] + dummy_variables, axis=1)\n",
    "        # df_synth = df_synth.drop(self.cat_features, axis=1)\n",
    "            \n",
    "        # X_balanced = pd.concat([X, df_synth.drop(self.target, axis=1)])\n",
    "        # y_balanced = pd.concat([y, df_synth[self.target]])\n",
    "        \n",
    "        X_df = pd.DataFrame(X, columns=self.num_features + self.cat_features)\n",
    "        y_df = pd.Series(y, name=self.target)\n",
    "        \n",
    "        X_balanced = pd.concat([X_df, df_synth.drop(self.target, axis=1)], axis=0, ignore_index=True)\n",
    "        y_balanced = pd.concat([y_df, df_synth[self.target]], axis=0, ignore_index=True)\n",
    "\n",
    "        return X_balanced, y_balanced\n",
    "    \n",
    "    def fit_resample(self, X, y):\n",
    "        print(\"Fit\")\n",
    "        output = self._fit_resample(X, y)\n",
    "        return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "05aa489f-6b41-4449-876d-290690b9b2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.utils.validation import check_X_y, check_array, check_is_fitted\n",
    "\n",
    "class SDVPipelineTBTR(BaseSampler):\n",
    "    _sampling_type = \"bypass\"\n",
    "    \n",
    "    def __init__(self, metadata, target, num_features, cat_features):\n",
    "        self.metadata = metadata\n",
    "        self.target = target\n",
    "        self.num_features = num_features\n",
    "        self.cat_features = cat_features\n",
    "        self.synthesizer = SingleTablePreset(self.metadata, name=\"FAST_ML\")\n",
    "        super().__init__()\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        X, y = check_X_y(X, y)\n",
    "        \n",
    "        df_train = pd.concat([X, pd.Series(y, name=self.target)], axis=1)\n",
    "        self.synthesizer.fit(df_train)\n",
    "        \n",
    "        self.class_counts_ = pd.Series(y).value_counts()\n",
    "        self.minority_class_ = self.class_counts_.idxmin()\n",
    "        self.synthetic_samples_needed_ = self.class_counts_.max() - self.class_counts_.min()\n",
    "\n",
    "        return self\n",
    "\n",
    "    def _fit_resample(self, X, y):\n",
    "        check_is_fitted(self, \"synthesizer\")\n",
    "        \n",
    "        if self.minority_class_ == 0:\n",
    "            balanced_conditions_0 = Condition(\n",
    "                num_rows=self.synthetic_samples_needed_,\n",
    "                column_values={self.target: 0},\n",
    "            )\n",
    "            df_synth = self.synthesizer.sample_from_conditions(\n",
    "                conditions=[balanced_conditions_0]\n",
    "            )\n",
    "        elif self.minority_class_ == 1:\n",
    "            balanced_conditions_1 = Condition(\n",
    "                num_rows=self.synthetic_samples_needed_,\n",
    "                column_values={self.target: 1},\n",
    "            )\n",
    "            df_synth = self.synthesizer.sample_from_conditions(\n",
    "                conditions=[balanced_conditions_1]\n",
    "            )\n",
    "\n",
    "        X_df = pd.DataFrame(X, columns=self.num_features + self.cat_features)\n",
    "        y_df = pd.Series(y, name=self.target)\n",
    "\n",
    "        X_balanced = pd.concat([X_df, df_synth.drop(self.target, axis=1)], axis=0, ignore_index=True)\n",
    "        y_balanced = pd.concat([y_df, df_synth[self.target]], axis=0, ignore_index=True)\n",
    "\n",
    "        return X_balanced, y_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "af80301b-3538-437e-8990-0b21c7edbf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline_tbtr(X, y, num_features: list, cat_features: list, metadata, target):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    num_outer_loop_folds = 5\n",
    "    num_inner_loop_folds = 5\n",
    "    results = []\n",
    "\n",
    "    models = [\n",
    "        (\n",
    "            \"SVM\",\n",
    "            SVC(),\n",
    "            {\"model__C\": [0.1, 0.5, 1, 5, 10], \"model__kernel\": [\"linear\", \"rbf\"]},\n",
    "        ),\n",
    "        (\n",
    "            \"Decision Tree\",\n",
    "            DecisionTreeClassifier(),\n",
    "            {\n",
    "                \"model__max_depth\": [None, 1, 2, 5, 10],\n",
    "                \"model__min_samples_split\": [2, 5, 10],\n",
    "                \"model__min_samples_leaf\": [1, 5],\n",
    "            },\n",
    "        ),\n",
    "        (\n",
    "            \"KNN\",\n",
    "            KNeighborsClassifier(),\n",
    "            {\n",
    "                \"model__n_neighbors\": [1, 3, 5, 7, 10],\n",
    "                \"model__weights\": [\"uniform\", \"distance\"],\n",
    "            },\n",
    "        ),\n",
    "        (\n",
    "            \"Random Forest\",\n",
    "            RandomForestClassifier(),\n",
    "            {\n",
    "                \"model__n_estimators\": [100, 200, 300],\n",
    "                \"model__max_depth\": [None, 5, 10],\n",
    "                \"model__min_samples_split\": [2, 5],\n",
    "                \"model__min_samples_leaf\": [1, 5],\n",
    "            },\n",
    "        ),\n",
    "        (\n",
    "            \"Logistic Regression\",\n",
    "            LogisticRegression(max_iter=1000),\n",
    "            {\n",
    "                \"model__C\": [0.1, 0.5, 1, 5, 10],\n",
    "                \"model__solver\": [\"liblinear\", \"sag\", \"saga\"],\n",
    "            },\n",
    "        ),\n",
    "        (\n",
    "            \"MLP\",\n",
    "            MLPClassifier(max_iter=1000),\n",
    "            {\n",
    "                \"model__hidden_layer_sizes\": [(100,), (100, 50)],\n",
    "                \"model__alpha\": [0.0001, 0.001, 0.01],\n",
    "            },\n",
    "        ),\n",
    "    ]\n",
    "\n",
    "    standard_scaler = ColumnTransformer(\n",
    "        transformers=[(\"numerical_standard_scaler\", StandardScaler(), num_features)],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "    \n",
    "    def create_dummy_transformer(columns):\n",
    "        return FunctionTransformer(dummy_transform, kw_args={'columns': columns})\n",
    "\n",
    "    dummy_transform_cats = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('one_hot_encoding', create_dummy_transformer(cat_features), cat_features),\n",
    "        ],\n",
    "        remainder='passthrough'\n",
    "    )\n",
    "\n",
    "    for name, model, param_grid in models:\n",
    "        print(f\"\\nTraining Model: {model}\")\n",
    "        folds = KFold(n_splits=num_outer_loop_folds, shuffle=True).split(X, y)\n",
    "        for i, (train_index, test_index) in enumerate(folds):\n",
    "            print(f\"Training fold {i+1}...\")\n",
    "\n",
    "            X_train, y_train = X.iloc[train_index, :], y.iloc[train_index]\n",
    "            X_test, y_test = X.iloc[test_index, :], y.iloc[test_index]\n",
    "\n",
    "            print(\n",
    "                f\"X_train: {X_train.shape} \\nX_test: {X_test.shape} \\ny_train: {y_train.shape} \\ny_test: {y_test.shape}\"\n",
    "            )\n",
    "\n",
    "            balance_classes_step = (\"balancing\", SDVPipelineTBTR(metadata, target, num_features, cat_features))\n",
    "            one_hot_encoder_step = (\"one_hot_encoding\", dummy_transform_cats, cat_features)\n",
    "            normalization_step = (\"normalization\", standard_scaler, num_features)\n",
    "            model_step = (\"model\", model)\n",
    "\n",
    "            steps = [balance_classes_step, one_hot_encoder_step, normalization_step, model_step]\n",
    "            pipeline = Pipeline(steps)\n",
    "\n",
    "            clf = GridSearchCV(pipeline, param_grid, cv=num_inner_loop_folds, n_jobs=-1, error_score='raise')\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = clf.predict(X_test)\n",
    "            accuracy_test = accuracy_score(y_test, y_pred)\n",
    "            f1_test = f1_score(y_test, y_pred)\n",
    "            precision_test = precision_score(y_test, y_pred)\n",
    "            recall_test = recall_score(y_test, y_pred)\n",
    "            roc_auc_test = roc_auc_score(y_test, y_pred)\n",
    "\n",
    "            results.append(\n",
    "                {\n",
    "                    \"Classification\": name,\n",
    "                    \"Accuracy\": round(accuracy_test, 4),\n",
    "                    \"Precision\": round(precision_test, 4),\n",
    "                    \"Recall\": round(recall_test, 4),\n",
    "                    \"F1 Score\": round(f1_test, 4),\n",
    "                    \"ROC/AUC\": round(roc_auc_test, 4),\n",
    "                }\n",
    "            )\n",
    "\n",
    "    mean_df = pd.DataFrame(results).groupby([\"Classification\"], as_index=False).mean()\n",
    "    std_df = pd.DataFrame(results).groupby([\"Classification\"], as_index=False).std()\n",
    "    results_df = mean_std_results(mean_df, std_df)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d5384da3-6014-499b-a47f-e896ce95152e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # TBTR (Treinar com Dados Balanceados [SDV + Dados Reais], Testar com Dados Reais)\n",
    "# class SDVPipelineTBTR(BaseEstimator, TransformerMixin):\n",
    "#     def __init__(self, metadata, target):\n",
    "#         self.metadata = metadata\n",
    "#         self.target = target\n",
    "#         self.synthesizer = SingleTablePreset(self.metadata, name=\"FAST_ML\")\n",
    "\n",
    "#     def fit(self, X_train, y_train):\n",
    "#         df_train = pd.concat([X_train, y_train], axis=1)\n",
    "#         self.synthesizer.fit(df_train)\n",
    "#         return self\n",
    "\n",
    "#     def resample(self, X_train, y_train):\n",
    "#         class_counts = y_train.value_counts()\n",
    "#         minority_class = class_counts.idxmin()\n",
    "#         synthetic_samples_needed = class_counts.max() - class_counts.min()\n",
    "\n",
    "#         if minority_class == 0:\n",
    "#             balanced_conditions_0 = Condition(\n",
    "#                 num_rows=synthetic_samples_needed,\n",
    "#                 column_values={self.target: 0},\n",
    "#             )\n",
    "#             df_synth = self.synthesizer.sample_from_conditions(\n",
    "#                 conditions=[balanced_conditions_0]\n",
    "#             )\n",
    "#         elif minority_class == 1:\n",
    "#             balanced_conditions_1 = Condition(\n",
    "#                 num_rows=synthetic_samples_needed,\n",
    "#                 column_values={self.target: 1},\n",
    "#             )\n",
    "#             df_synth = self.synthesizer.sample_from_conditions(\n",
    "#                 conditions=[balanced_conditions_1]\n",
    "#             )\n",
    "            \n",
    "#         X_train_balanced = pd.concat([X_train, df_synth.drop(self.target, axis=1)])\n",
    "#         y_train_balanced = pd.concat([y_train, df_synth[self.target]])\n",
    "\n",
    "#         return X_train_balanced, y_train_balanced"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f42b805a-452c-4f51-bc8b-9542264ca4d8",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'BaseEstimator' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[14], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# TSTR (Treinar com Dados Sintéticos, Testar com Dados Reais)\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Dados Sintéticos Balanceados\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m \u001b[38;5;21;01mSDVPipelineTSTR\u001b[39;00m(\u001b[43mBaseEstimator\u001b[49m, TransformerMixin):\n\u001b[0;32m      4\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, metadata, target):\n\u001b[0;32m      5\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m=\u001b[39m metadata\n",
      "\u001b[1;31mNameError\u001b[0m: name 'BaseEstimator' is not defined"
     ]
    }
   ],
   "source": [
    "# TSTR (Treinar com Dados Sintéticos, Testar com Dados Reais)\n",
    "# Dados Sintéticos Balanceados\n",
    "class SDVPipelineTSTR(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, metadata, target):\n",
    "        self.metadata = metadata\n",
    "        self.synthesizer = SingleTablePreset(self.metadata, name=\"FAST_ML\")\n",
    "\n",
    "    def fit(self, X_train, y_train):\n",
    "        df_train = pd.concat([X_train, y_train], axis=1)\n",
    "        self.synthesizer.fit(df_train)\n",
    "        return self\n",
    "\n",
    "    def resample(self, X_train, y_train, target):\n",
    "        class_counts = y_train.value_counts()\n",
    "        majority_class = class_counts.idxmax()\n",
    "        synthetic_samples_needed = class_counts.max()\n",
    "\n",
    "        balanced_conditions_0 = Condition(\n",
    "            num_rows=synthetic_samples_needed,\n",
    "            column_values={target: 0},\n",
    "        )\n",
    "\n",
    "        balanced_conditions_1 = Condition(\n",
    "            num_rows=synthetic_samples_needed,\n",
    "            column_values={target: 1},\n",
    "        )\n",
    "\n",
    "        df_synth = self.synthesizer.sample_from_conditions(\n",
    "            conditions=[balanced_conditions_0, balanced_conditions_1]\n",
    "        )\n",
    "\n",
    "        # onehotencoder\n",
    "        # dummy_variables = dummy(df_synth, cat_features)\n",
    "        # df_synth = pd.concat([df_synth] + dummy_variables, axis=1)\n",
    "        # df_synth = df_synth.drop(cat_features, axis=1)\n",
    "\n",
    "        X_train_synth = df_synth.drop(target, axis=1)\n",
    "        y_train_synth = df_synth[target]\n",
    "\n",
    "        return X_train_synth, y_train_synth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2cf7b3d3-ec02-4500-acdd-19feb6736f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline_tbtr(X, y, num_features: list , cat_features: list, metadata, target):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    num_outer_loop_folds = 5\n",
    "    num_inner_loop_folds = 5\n",
    "    results = []\n",
    "    \n",
    "    models = [\n",
    "        (\n",
    "            \"SVM\",\n",
    "            SVC(),\n",
    "            {\"C\": [0.1, 0.5, 1, 5, 10], \"kernel\": [\"linear\", \"rbf\"]},\n",
    "        ),\n",
    "        (\n",
    "            \"Decision Tree\",\n",
    "            DecisionTreeClassifier(),\n",
    "            {\n",
    "                \"model__max_depth\": [None, 1, 2, 5, 10],\n",
    "                \"model__min_samples_split\": [2, 5, 10],\n",
    "                \"model__min_samples_leaf\": [1, 5],\n",
    "            },\n",
    "        ),\n",
    "        (\n",
    "            \"KNN\",\n",
    "            KNeighborsClassifier(),\n",
    "            {\n",
    "                \"model__n_neighbors\": [1, 3, 5, 7, 10],\n",
    "                \"model__weights\": [\"uniform\", \"distance\"],\n",
    "            },\n",
    "        ),\n",
    "        (\n",
    "            \"Random Forest\",\n",
    "            RandomForestClassifier(),\n",
    "            {\n",
    "                \"model__n_estimators\": [100, 200, 300],\n",
    "                \"model__max_depth\": [None, 5, 10],\n",
    "                \"model__min_samples_split\": [2, 5],\n",
    "                \"model__min_samples_leaf\": [1, 5],\n",
    "            },\n",
    "        ),\n",
    "        (\n",
    "            \"Logistic Regression\",\n",
    "            LogisticRegression(max_iter=1000),\n",
    "            {\n",
    "                \"model__C\": [0.1, 0.5, 1, 5, 10],\n",
    "                \"model__solver\": [\"liblinear\", \"sag\", \"saga\"],\n",
    "            },\n",
    "        ),\n",
    "        (\n",
    "            \"MLP\",\n",
    "            MLPClassifier(max_iter=1000),\n",
    "            {\n",
    "                \"model__hidden_layer_sizes\": [(100,), (100, 50)],\n",
    "                \"model__alpha\": [0.0001, 0.001, 0.01],\n",
    "            },\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    standard_scaler = ColumnTransformer(\n",
    "        transformers=[(\"numerical_standard_scaler\", StandardScaler(), num_features)],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "    \n",
    "\n",
    "    for name, model, param_grid in models:\n",
    "        print(f\"\\nTraining Model: {model}\")\n",
    "        folds = KFold(n_splits=num_outer_loop_folds, shuffle=True).split(X, y)\n",
    "        for i, (train_index, test_index) in enumerate(folds):\n",
    "            print(f\"Training fold {i+1}...\")\n",
    "       \n",
    "            X_train, y_train = X.iloc[train_index, :], y.iloc[train_index]\n",
    "            X_test, y_test = X.iloc[test_index, :], y.iloc[test_index]\n",
    "            \n",
    "            print(\n",
    "                f\"X_train: {X_train.shape} \\nX_test: {X_test.shape} \\ny_train: {y_train.shape} \\ny_test: {y_test.shape}\"\n",
    "            )\n",
    "             \n",
    "            balance_classes_step = (\"balacing\", SDVPipelineTBTR(metadata, target, num_features, cat_features))\n",
    "            normalization_step = (\"normalization\", standard_scaler)\n",
    "            model_step = (\"model\", model)\n",
    "            steps = [balance_classes_step, normalization_step, model_step]\n",
    "            pipeline = Pipeline(steps)\n",
    "\n",
    "            clf = GridSearchCV(pipeline, param_grid, cv=num_inner_loop_folds, n_jobs=-1, error_score='raise')\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = clf.predict(X_test)\n",
    "            accuracy_test = accuracy_score(y_test, y_pred)\n",
    "            f1_test = f1_score(y_test, y_pred)\n",
    "            precision_test = precision_score(y_test, y_pred)\n",
    "            recall_test = recall_score(y_test, y_pred)\n",
    "            roc_auc_test = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "            results.append(\n",
    "                {\n",
    "                    \"Classification\": name,\n",
    "                    \"Accuracy\": round(accuracy_test, 4),\n",
    "                    \"Precision\": round(precision_test, 4),\n",
    "                    \"Recall\": round(recall_test, 4),\n",
    "                    \"F1 Score\": round(f1_test, 4),\n",
    "                    \"ROC/AUC\": round(roc_auc_test, 4),\n",
    "                }\n",
    "            )\n",
    "        \n",
    "    mean_df = pd.DataFrame(results).groupby([\"Classification\"], as_index = False).mean()\n",
    "    std_df = pd.DataFrame(results).groupby([\"Classification\"], as_index = False).std()\n",
    "    results_df = mean_std_results(mean_df, std_df)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e9250bcb-b7d8-423a-b452-2d1ce727d902",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_pipeline_smote(X, y, num_features: list , cat_features: list):\n",
    "    np.random.seed(42)\n",
    "\n",
    "    num_outer_loop_folds = 5\n",
    "    num_inner_loop_folds = 5\n",
    "    results = []\n",
    "    \n",
    "    models = [\n",
    "        (\n",
    "            \"SVM\",\n",
    "            SVC(),\n",
    "            {\"model__C\": [0.1, 0.5, 1, 5, 10], \"model__kernel\": [\"linear\", \"rbf\"]},\n",
    "        ),\n",
    "        (\n",
    "            \"Decision Tree\",\n",
    "            DecisionTreeClassifier(),\n",
    "            {\n",
    "                \"model__max_depth\": [None, 1, 2, 5, 10],\n",
    "                \"model__min_samples_split\": [2, 5, 10],\n",
    "                \"model__min_samples_leaf\": [1, 5],\n",
    "            },\n",
    "        ),\n",
    "        (\n",
    "            \"KNN\",\n",
    "            KNeighborsClassifier(),\n",
    "            {\n",
    "                \"model__n_neighbors\": [1, 3, 5, 7, 10],\n",
    "                \"model__weights\": [\"uniform\", \"distance\"],\n",
    "            },\n",
    "        ),\n",
    "        (\n",
    "            \"Random Forest\",\n",
    "            RandomForestClassifier(),\n",
    "            {\n",
    "                \"model__n_estimators\": [100, 200, 300],\n",
    "                \"model__max_depth\": [None, 5, 10],\n",
    "                \"model__min_samples_split\": [2, 5],\n",
    "                \"model__min_samples_leaf\": [1, 5],\n",
    "            },\n",
    "        ),\n",
    "        (\n",
    "            \"Logistic Regression\",\n",
    "            LogisticRegression(max_iter=1000),\n",
    "            {\n",
    "                \"model__C\": [0.1, 0.5, 1, 5, 10],\n",
    "                \"model__solver\": [\"liblinear\", \"sag\", \"saga\"],\n",
    "            },\n",
    "        ),\n",
    "        (\n",
    "            \"MLP\",\n",
    "            MLPClassifier(max_iter=1000),\n",
    "            {\n",
    "                \"model__hidden_layer_sizes\": [(100,), (100, 50)],\n",
    "                \"model__alpha\": [0.0001, 0.001, 0.01],\n",
    "            },\n",
    "        ),\n",
    "    ]\n",
    "    \n",
    "    standard_scaler = ColumnTransformer(\n",
    "        transformers=[(\"numerical_standard_scaler\", StandardScaler(), num_features)],\n",
    "        remainder=\"passthrough\",\n",
    "    )\n",
    "        \n",
    "    for name, model, param_grid in models:\n",
    "        print(f\"\\nTraining Model: {model}\")\n",
    "        folds = KFold(n_splits=num_outer_loop_folds, shuffle=True).split(X, y)\n",
    "        for i, (train_index, test_index) in enumerate(folds):\n",
    "            print(f\"Training fold {i+1}...\")\n",
    "       \n",
    "            X_train, y_train = X.iloc[train_index, :], y.iloc[train_index]\n",
    "            X_test, y_test = X.iloc[test_index, :], y.iloc[test_index]\n",
    "          \n",
    "            balance_classes_step = (\"balacing\", SMOTE(random_state=42))\n",
    "            normalization_step = (\"normalization\", standard_scaler)\n",
    "            model_step = (\"model\", model)\n",
    "            steps = [normalization_step, balance_classes_step, model_step]\n",
    "            pipeline = Pipeline(steps)\n",
    "\n",
    "            clf = GridSearchCV(pipeline, param_grid, cv=num_inner_loop_folds, n_jobs=-1)\n",
    "            clf.fit(X_train, y_train)\n",
    "\n",
    "            y_pred = clf.predict(X_test)\n",
    "            accuracy_test = accuracy_score(y_test, y_pred)\n",
    "            f1_test = f1_score(y_test, y_pred)\n",
    "            precision_test = precision_score(y_test, y_pred)\n",
    "            recall_test = recall_score(y_test, y_pred)\n",
    "            roc_auc_test = roc_auc_score(y_test, y_pred)\n",
    "    \n",
    "            results.append(\n",
    "                {\n",
    "                    \"Classification\": name,\n",
    "                    \"Accuracy\": round(accuracy_test, 4),\n",
    "                    \"Precision\": round(precision_test, 4),\n",
    "                    \"Recall\": round(recall_test, 4),\n",
    "                    \"F1 Score\": round(f1_test, 4),\n",
    "                    \"ROC/AUC\": round(roc_auc_test, 4),\n",
    "                }\n",
    "            )\n",
    "    mean_df = pd.DataFrame(results).groupby([\"Classification\"], as_index = False).mean()\n",
    "    std_df = pd.DataFrame(results).groupby([\"Classification\"], as_index = False).std()\n",
    "    results_df = mean_std_results(mean_df, std_df)\n",
    "    return results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3ae738-0464-4ccc-b31a-6fbfdfdcc85b",
   "metadata": {},
   "source": [
    "## Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "65f3b389-1178-416c-a633-2afffd5d6d29",
   "metadata": {},
   "outputs": [],
   "source": [
    "cat_features = [\"Education\", \"Gender\", \"City\", \"EverBenched\", \"PaymentTier\", \"ExperienceInCurrentDomain\"]\n",
    "num_features = [\"Age\", \"JoiningYear\"]\n",
    "target = \"LeaveOrNot\"\n",
    "\n",
    "df = pd.read_csv(\"data/employee/employee.csv\").sample(50)\n",
    "\n",
    "# df = dummy_transform(df, cat_features)\n",
    "\n",
    "y = df[target]\n",
    "X = df.drop(target, axis=1)\n",
    "\n",
    "metadata = SingleTableMetadata()\n",
    "metadata.detect_from_dataframe(data=df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7b40ef5-6c8e-4c31-90a1-209b3087489a",
   "metadata": {},
   "source": [
    "## TBTR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "711edd2a-106e-4dc0-b447-4bac4753cf78",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model: SVC()\n",
      "Training fold 1...\n",
      "X_train: (40, 8) \n",
      "X_test: (10, 8) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Invalid parameter 'model' for estimator Pipeline(steps=[('balancing',\n                 SDVPipelineTBTR(cat_features=['Education', 'Gender', 'City',\n                                               'EverBenched', 'PaymentTier',\n                                               'ExperienceInCurrentDomain'],\n                                 metadata={\n    \"columns\": {\n        \"Education\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"JoiningYear\": {\n            \"sdtype\": \"numerical\"\n        },\n        \"City\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"PaymentTier\": {\n            \"sdtype\": \"numerical\"\n        },\n        \"Age\": {\n            \"sdtype\": \"numerical\"\n        },\n        \"Gender\": {\n            \"sd...\n                                                  ['Education', 'Gender',\n                                                   'City', 'EverBenched',\n                                                   'PaymentTier',\n                                                   'ExperienceInCurrentDomain'])]),\n                 ['Education', 'Gender', 'City', 'EverBenched', 'PaymentTier',\n                  'ExperienceInCurrentDomain']),\n                ('normalization',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('numerical_standard_scaler',\n                                                  StandardScaler(),\n                                                  ['Age', 'JoiningYear'])]),\n                 ['Age', 'JoiningYear']),\n                ('model', SVC())]). Valid parameters are: ['memory', 'steps', 'verbose'].",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31m_RemoteTraceback\u001b[0m                          Traceback (most recent call last)",
      "\u001b[1;31m_RemoteTraceback\u001b[0m: \n\"\"\"\nTraceback (most recent call last):\n  File \"C:\\Users\\caio_barros\\AppData\\Local\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 436, in _process_worker\n    r = call_item()\n  File \"C:\\Users\\caio_barros\\AppData\\Local\\anaconda3\\lib\\site-packages\\joblib\\externals\\loky\\process_executor.py\", line 288, in __call__\n    return self.fn(*self.args, **self.kwargs)\n  File \"C:\\Users\\caio_barros\\AppData\\Local\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py\", line 595, in __call__\n    return self.func(*args, **kwargs)\n  File \"C:\\Users\\caio_barros\\AppData\\Local\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in __call__\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\caio_barros\\AppData\\Local\\anaconda3\\lib\\site-packages\\joblib\\parallel.py\", line 263, in <listcomp>\n    return [func(*args, **kwargs)\n  File \"C:\\Users\\caio_barros\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py\", line 123, in __call__\n    return self.function(*args, **kwargs)\n  File \"C:\\Users\\caio_barros\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 674, in _fit_and_score\n    estimator = estimator.set_params(**cloned_parameters)\n  File \"C:\\Users\\caio_barros\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\pipeline.py\", line 211, in set_params\n    self._set_params(\"steps\", **kwargs)\n  File \"C:\\Users\\caio_barros\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\utils\\metaestimators.py\", line 70, in _set_params\n    super().set_params(**params)\n  File \"C:\\Users\\caio_barros\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\base.py\", line 205, in set_params\n    raise ValueError(\nValueError: Invalid parameter 'model' for estimator Pipeline(steps=[('balancing',\n                 SDVPipelineTBTR(cat_features=['Education', 'Gender', 'City',\n                                               'EverBenched', 'PaymentTier',\n                                               'ExperienceInCurrentDomain'],\n                                 metadata={\n    \"columns\": {\n        \"Education\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"JoiningYear\": {\n            \"sdtype\": \"numerical\"\n        },\n        \"City\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"PaymentTier\": {\n            \"sdtype\": \"numerical\"\n        },\n        \"Age\": {\n            \"sdtype\": \"numerical\"\n        },\n        \"Gender\": {\n            \"sd...\n                                                  ['Education', 'Gender',\n                                                   'City', 'EverBenched',\n                                                   'PaymentTier',\n                                                   'ExperienceInCurrentDomain'])]),\n                 ['Education', 'Gender', 'City', 'EverBenched', 'PaymentTier',\n                  'ExperienceInCurrentDomain']),\n                ('normalization',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('numerical_standard_scaler',\n                                                  StandardScaler(),\n                                                  ['Age', 'JoiningYear'])]),\n                 ['Age', 'JoiningYear']),\n                ('model', SVC())]). Valid parameters are: ['memory', 'steps', 'verbose'].\n\"\"\"",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[27], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m results_df \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate_pipeline_tbtr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcat_features\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtarget\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      2\u001b[0m results_df\n",
      "Cell \u001b[1;32mIn[25], line 96\u001b[0m, in \u001b[0;36mevaluate_pipeline_tbtr\u001b[1;34m(X, y, num_features, cat_features, metadata, target)\u001b[0m\n\u001b[0;32m     93\u001b[0m pipeline \u001b[38;5;241m=\u001b[39m Pipeline(steps)\n\u001b[0;32m     95\u001b[0m clf \u001b[38;5;241m=\u001b[39m GridSearchCV(pipeline, param_grid, cv\u001b[38;5;241m=\u001b[39mnum_inner_loop_folds, n_jobs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, error_score\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mraise\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 96\u001b[0m \u001b[43mclf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     98\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m clf\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     99\u001b[0m accuracy_test \u001b[38;5;241m=\u001b[39m accuracy_score(y_test, y_pred)\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:874\u001b[0m, in \u001b[0;36mBaseSearchCV.fit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    868\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_results(\n\u001b[0;32m    869\u001b[0m         all_candidate_params, n_splits, all_out, all_more_results\n\u001b[0;32m    870\u001b[0m     )\n\u001b[0;32m    872\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m results\n\u001b[1;32m--> 874\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run_search\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluate_candidates\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    876\u001b[0m \u001b[38;5;66;03m# multimetric is determined here because in the case of a callable\u001b[39;00m\n\u001b[0;32m    877\u001b[0m \u001b[38;5;66;03m# self.scoring the return type is only known after calling\u001b[39;00m\n\u001b[0;32m    878\u001b[0m first_test_score \u001b[38;5;241m=\u001b[39m all_out[\u001b[38;5;241m0\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtest_scores\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:1388\u001b[0m, in \u001b[0;36mGridSearchCV._run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1386\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_run_search\u001b[39m(\u001b[38;5;28mself\u001b[39m, evaluate_candidates):\n\u001b[0;32m   1387\u001b[0m     \u001b[38;5;124;03m\"\"\"Search all candidates in param_grid\"\"\"\u001b[39;00m\n\u001b[1;32m-> 1388\u001b[0m     \u001b[43mevaluate_candidates\u001b[49m\u001b[43m(\u001b[49m\u001b[43mParameterGrid\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparam_grid\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_search.py:821\u001b[0m, in \u001b[0;36mBaseSearchCV.fit.<locals>.evaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mverbose \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    814\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\n\u001b[0;32m    815\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFitting \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m folds for each of \u001b[39m\u001b[38;5;132;01m{1}\u001b[39;00m\u001b[38;5;124m candidates,\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    816\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m totalling \u001b[39m\u001b[38;5;132;01m{2}\u001b[39;00m\u001b[38;5;124m fits\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(\n\u001b[0;32m    817\u001b[0m             n_splits, n_candidates, n_candidates \u001b[38;5;241m*\u001b[39m n_splits\n\u001b[0;32m    818\u001b[0m         )\n\u001b[0;32m    819\u001b[0m     )\n\u001b[1;32m--> 821\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mparallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    822\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_fit_and_score\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    823\u001b[0m \u001b[43m        \u001b[49m\u001b[43mclone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbase_estimator\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    824\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    826\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtrain\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    827\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtest\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtest\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    828\u001b[0m \u001b[43m        \u001b[49m\u001b[43mparameters\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mparameters\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    829\u001b[0m \u001b[43m        \u001b[49m\u001b[43msplit_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_splits\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    830\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcandidate_progress\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_candidates\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    831\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_and_score_kwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    832\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    833\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mcand_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparameters\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43msplit_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mproduct\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    834\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcandidate_params\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msplit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    836\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    838\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(out) \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[0;32m    839\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    840\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo fits were performed. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    841\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWas the CV iterator empty? \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    842\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWere there no candidates?\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    843\u001b[0m     )\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\sklearn\\utils\\parallel.py:63\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m     58\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config()\n\u001b[0;32m     59\u001b[0m iterable_with_config \u001b[38;5;241m=\u001b[39m (\n\u001b[0;32m     60\u001b[0m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[0;32m     62\u001b[0m )\n\u001b[1;32m---> 63\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:1061\u001b[0m, in \u001b[0;36mParallel.__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1058\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterating \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m   1060\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend\u001b[38;5;241m.\u001b[39mretrieval_context():\n\u001b[1;32m-> 1061\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mretrieve\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1062\u001b[0m \u001b[38;5;66;03m# Make sure that we get a last message telling us we are done\u001b[39;00m\n\u001b[0;32m   1063\u001b[0m elapsed_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_start_time\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\joblib\\parallel.py:938\u001b[0m, in \u001b[0;36mParallel.retrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    936\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m    937\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backend, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msupports_timeout\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[1;32m--> 938\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(\u001b[43mjob\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m    939\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    940\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output\u001b[38;5;241m.\u001b[39mextend(job\u001b[38;5;241m.\u001b[39mget())\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\joblib\\_parallel_backends.py:542\u001b[0m, in \u001b[0;36mLokyBackend.wrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    539\u001b[0m \u001b[38;5;124;03m\"\"\"Wrapper for Future.result to implement the same behaviour as\u001b[39;00m\n\u001b[0;32m    540\u001b[0m \u001b[38;5;124;03mAsyncResults.get from multiprocessing.\"\"\"\u001b[39;00m\n\u001b[0;32m    541\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 542\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfuture\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mresult\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    543\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m CfTimeoutError \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    544\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\concurrent\\futures\\_base.py:458\u001b[0m, in \u001b[0;36mFuture.result\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    456\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m CancelledError()\n\u001b[0;32m    457\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_state \u001b[38;5;241m==\u001b[39m FINISHED:\n\u001b[1;32m--> 458\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m__get_result\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    460\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m()\n",
      "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\concurrent\\futures\\_base.py:403\u001b[0m, in \u001b[0;36mFuture.__get_result\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception:\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 403\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_exception\n\u001b[0;32m    404\u001b[0m     \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    405\u001b[0m         \u001b[38;5;66;03m# Break a reference cycle with the exception in self._exception\u001b[39;00m\n\u001b[0;32m    406\u001b[0m         \u001b[38;5;28mself\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Invalid parameter 'model' for estimator Pipeline(steps=[('balancing',\n                 SDVPipelineTBTR(cat_features=['Education', 'Gender', 'City',\n                                               'EverBenched', 'PaymentTier',\n                                               'ExperienceInCurrentDomain'],\n                                 metadata={\n    \"columns\": {\n        \"Education\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"JoiningYear\": {\n            \"sdtype\": \"numerical\"\n        },\n        \"City\": {\n            \"sdtype\": \"categorical\"\n        },\n        \"PaymentTier\": {\n            \"sdtype\": \"numerical\"\n        },\n        \"Age\": {\n            \"sdtype\": \"numerical\"\n        },\n        \"Gender\": {\n            \"sd...\n                                                  ['Education', 'Gender',\n                                                   'City', 'EverBenched',\n                                                   'PaymentTier',\n                                                   'ExperienceInCurrentDomain'])]),\n                 ['Education', 'Gender', 'City', 'EverBenched', 'PaymentTier',\n                  'ExperienceInCurrentDomain']),\n                ('normalization',\n                 ColumnTransformer(remainder='passthrough',\n                                   transformers=[('numerical_standard_scaler',\n                                                  StandardScaler(),\n                                                  ['Age', 'JoiningYear'])]),\n                 ['Age', 'JoiningYear']),\n                ('model', SVC())]). Valid parameters are: ['memory', 'steps', 'verbose']."
     ]
    }
   ],
   "source": [
    "results_df = evaluate_pipeline_tbtr(X, y, num_features, cat_features, metadata, target)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71fca3cf-2fb3-4f2f-af37-be33e97286d2",
   "metadata": {},
   "source": [
    "## SMOTE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "bc2f4e7c-250d-4dfe-82ef-e1111d47d8ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training Model: SVC()\n",
      "Training fold 1...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 2...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 3...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 4...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 5...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "\n",
      "Training Model: DecisionTreeClassifier()\n",
      "Training fold 1...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 2...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 3...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 4...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 5...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "\n",
      "Training Model: KNeighborsClassifier()\n",
      "Training fold 1...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 2...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 3...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 4...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 5...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "\n",
      "Training Model: RandomForestClassifier()\n",
      "Training fold 1...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 2...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 3...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 4...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 5...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "\n",
      "Training Model: LogisticRegression(max_iter=1000)\n",
      "Training fold 1...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 2...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 3...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 4...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 5...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "\n",
      "Training Model: MLPClassifier(max_iter=1000)\n",
      "Training fold 1...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 2...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 3...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 4...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n",
      "Training fold 5...\n",
      "X_train: (40, 22) \n",
      "X_test: (10, 22) \n",
      "y_train: (40,) \n",
      "y_test: (10,)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Classification</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1 Score</th>\n",
       "      <th>ROC/AUC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Decision Tree</td>\n",
       "      <td>0.6400 +- 0.05</td>\n",
       "      <td>0.3800 +- 0.22</td>\n",
       "      <td>0.3500 +- 0.25</td>\n",
       "      <td>0.3467 +- 0.21</td>\n",
       "      <td>0.5456 +- 0.09</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>KNN</td>\n",
       "      <td>0.6200 +- 0.13</td>\n",
       "      <td>0.5238 +- 0.28</td>\n",
       "      <td>0.5833 +- 0.30</td>\n",
       "      <td>0.4543 +- 0.08</td>\n",
       "      <td>0.6274 +- 0.10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>0.7000 +- 0.16</td>\n",
       "      <td>0.5167 +- 0.39</td>\n",
       "      <td>0.5333 +- 0.45</td>\n",
       "      <td>0.4289 +- 0.29</td>\n",
       "      <td>0.6816 +- 0.21</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>MLP</td>\n",
       "      <td>0.6800 +- 0.11</td>\n",
       "      <td>0.5133 +- 0.37</td>\n",
       "      <td>0.4067 +- 0.38</td>\n",
       "      <td>0.3857 +- 0.25</td>\n",
       "      <td>0.6070 +- 0.16</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Random Forest</td>\n",
       "      <td>0.7400 +- 0.11</td>\n",
       "      <td>0.4667 +- 0.38</td>\n",
       "      <td>0.4667 +- 0.36</td>\n",
       "      <td>0.4400 +- 0.31</td>\n",
       "      <td>0.6673 +- 0.18</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>SVM</td>\n",
       "      <td>0.7400 +- 0.15</td>\n",
       "      <td>0.7133 +- 0.28</td>\n",
       "      <td>0.5667 +- 0.25</td>\n",
       "      <td>0.5698 +- 0.10</td>\n",
       "      <td>0.6917 +- 0.14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Classification        Accuracy       Precision          Recall  \\\n",
       "0        Decision Tree  0.6400 +- 0.05  0.3800 +- 0.22  0.3500 +- 0.25   \n",
       "1                  KNN  0.6200 +- 0.13  0.5238 +- 0.28  0.5833 +- 0.30   \n",
       "2  Logistic Regression  0.7000 +- 0.16  0.5167 +- 0.39  0.5333 +- 0.45   \n",
       "3                  MLP  0.6800 +- 0.11  0.5133 +- 0.37  0.4067 +- 0.38   \n",
       "4        Random Forest  0.7400 +- 0.11  0.4667 +- 0.38  0.4667 +- 0.36   \n",
       "5                  SVM  0.7400 +- 0.15  0.7133 +- 0.28  0.5667 +- 0.25   \n",
       "\n",
       "         F1 Score         ROC/AUC  \n",
       "0  0.3467 +- 0.21  0.5456 +- 0.09  \n",
       "1  0.4543 +- 0.08  0.6274 +- 0.10  \n",
       "2  0.4289 +- 0.29  0.6816 +- 0.21  \n",
       "3  0.3857 +- 0.25  0.6070 +- 0.16  \n",
       "4  0.4400 +- 0.31  0.6673 +- 0.18  \n",
       "5  0.5698 +- 0.10  0.6917 +- 0.14  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df = evaluate_pipeline_smote(X, y, num_features, cat_features)\n",
    "results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49de0ba8-4115-44a6-b161-13ea28b74427",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
